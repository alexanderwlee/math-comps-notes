\part*{Linear Algebra}

\section*{Vector Spaces and Subspaces}

\begin{definition}
	A (real) \emph{vector space} is a set $V$ (whose elements are called
	\emph{vectors}) together with
	\begin{enumerate}
		\item an operation called \emph{vector addition}, which for each pair of
			vector $\vec{x}, \vec{y} \in V$ produces another vector in $V$ denoted
			$\vec{x} + \vec{y}$, and
		\item an operation called \emph{multiplication by a scalar} (a real number),
			which for each vector $\vec{x} \in V$, and each scalar $c \in \R$
			produces another vector in $V$ denoted $c \vec{x}$.
	\end{enumerate}
	Furthermore, the two operations must satisfy the follow \emph{axioms}:
	\begin{enumerate}
		\item For all vectors $\vec{x}, \vec{y}$, and $\vec{z} \in V$, $(\vec{x}
			+ \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z})$.
		\item For all vectors $\vec{x}$ and $\vec{y} \in V$, $\vec{x} + \vec{y}
			= \vec{y} + \vec{x}$.
		\item There exists a vector $\vec{0} \in V$ with the property that
			$\vec{x} + \vec{0} = \vec{x}$ for all vectors $\vec{x} \in V$.
		\item For each vector $\vec{x} \in V$, there exists a vector denoted
			$-\vec{x}$ with the property that $\vec{x} + -\vec{x} = \vec{0}$.
		\item For all vectors $\vec{x}$ and $\vec{y} \in V$ and all scalars $c \in
			\R$, $c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$.
		\item For all vectors $\vec{x} \in V$, and all scalars $c$ and $d \in \R$,
			$(c + d)\vec{x} = c\vec{x} + d\vec{x}$.
		\item For all vectors $\vec{x} \in V$, and all scalars $c$ and $d \in \R$,
			$(cd)\vec{x} = c(d\vec{x})$.
		\item For all vectors $\vec{x} \in V$, $1\vec{x} = \vec{x}$.
	\end{enumerate}
\end{definition}

\begin{examples}
	Some simple vector spaces:
	\begin{itemize}
		\item $\R^n$ is the vector space of ordered $n$-tuples of real numbers.
			Note: $\dim(\R^n) = n$.
		\item $P_n(\R)$ is the vector space of polynomials of degree \emph{less
			than or equal to $n$}. Note: $\dim(P_n(\R)) = n + 1$.
		\item $M_{m \times n}(\R)$ is the vector space of $m \times n$ matrices with
			real entries. Note: $\dim(M_{m \times n}(\R)) = mn$.
	\end{itemize}
\end{examples}

\begin{definition}
	Let $V$ be a vector space and let $W \subseteq V$ be a subset. Then $W$ is a
	(vector) \emph{subspace} of $V$ if $W$ is a vector space itself under the
	operations of vector sum and scalar multiplication from $V$.
\end{definition}

\begin{notes}
	The empty set $\emptyset$ is not a vector space. Instead the smallest vector
	space is the trivial space, $\{\vec{0}\}$. Every vector space $V$ has two
	obvious subspaces: the trivial subspace $\{\vec{0}\} \subseteq V$, and the
	improper subspace $V \subseteq V$.
\end{notes}

\begin{theorem}[Subspace Theorem]
	Let $V$ be a vector space. A subset $W \subseteq V$ is a subspace if it
	satisfies the following properties:
	\begin{enumerate}
		\item $W \neq \emptyset$
		\item For all $\vec{x}, \vec{y} \in W$ and all $c \in \R$, we have $c\vec{x} +
			\vec{y} \in W$.
	\end{enumerate}
\end{theorem}

\begin{definition}
	Let $V$ be a vector space, and let $S = \{\vec{v}_1, \dots, \vec{v}_n\}
	\subseteq V$ be a finite set of vectors in $V$.
	\begin{itemize}
		\item A \emph{linear combination} of elements of $S$ is an expression $a_1
			\vec{v}_1 + \cdots + a_n \vec{v}_n$ for some scalars $a_1, \dots, a_n
			\in \R$.
		\item The \emph{span} of $S$, denoted $\Span(S)$, is the set of all linear
			combinations of elements of $S$. That is, \[\Span(S) = \{a_1 \vec{v}_1
			+ \cdots a_n \vec{v}_n \mid a_1, \dots, a_n \in \R\}.\]
		\item We define $\Span(\emptyset) = \{\vec{0}\}$.
		\item If $\Span(S) = W$, we say that $S$ spans $W$.
	\end{itemize}
\end{definition}

\begin{fact}
	Let $V$ be a vector space and let $S$ be any subset of $V$. Then
	$\Span(S)$ is a subspace of $V$.
\end{fact}

\begin{fact}
	If $W$ is a subspace and $S \subseteq W$, then $\Span(S) \subseteq W$.
\end{fact}

\begin{definition}
	The set $S$ is \emph{linearly dependent} if there exists scalars $a_1, \dots,
	a_n \in \R$ that are not all zero such that $a_1 \vec{v}_1 + \cdots + a_n
	\vec{v}_n = \vec{0}$. $S$ is \emph{linearly independent} if it is not
	linearly dependent. Equivalently, for any scalars $a_1, \dots, a_n \in \R$
	such that $a_1 \vec{v}_1 + \cdots a_n \vec{v}_n = \vec{0}$, we must have
	$a_1 = \cdots = a_n = 0$.
\end{definition}

\begin{definition}
	The set $S \subseteq V$ is a basis for $V$ if $S$ is linearly independent
	and $\Span(S) = V$.
\end{definition}

\begin{definition}
	The \emph{dimension} of $V$ is the number $\dim(V)$ of elements in a basis
	for $V$. If $V$ has no finite basis, we say $\dim(V) = \infty$.
\end{definition}

\begin{theorem}
	Any two bases of $V$ have the same number of elements.
\end{theorem}

\begin{fact}
	The three kinds of row reduction steps are
	\begin{enumerate}
		\item Switching two rows.
		\item Multiplying a row by a nonzero scalar.
		\item Adding a multiple of one row to another.
	\end{enumerate}
\end{fact}

\begin{definition}
	A matrix is in \emph{echelon form} if it satisfies all of the following
	conditions:
	\begin{enumerate}
		\item If a row is not a zero row (i.e., all entries of that row are zeros),
			then the first nonzero entry is a 1 (and called the \emph{pivot}).
		\item If a column contains a pivot, then all other entries in that column
			are 0.
		\item If a row contains a pivot, then each row above contains a pivot
			further to the left. This also implies that zero rows, if any, appear at
			the bottom.
	\end{enumerate}
	Variables corresponding to the pivots are called \emph{pivot variables}.
	All other variables are called \emph{free variables}.
\end{definition}

\begin{definition}
	A \emph{homogeneous} system of linear equations is when all the linear
	combinations equal 0. A system is \emph{inhomogeneous} otherwise.
\end{definition}

\begin{definition}
	The \emph{nullspace} of a matrix $A$ is the solution set of its
	corresponding homogeneous system of equations. The basis of the nullspace
	of $A$ is the set of vectors that the free variables end up multiplied by in
	the solution.
\end{definition}

\begin{definition}
	The \emph{column space} of a matrix $A$ is the span of its columns. If $B$ is
	the echelon form of $A$, then the columns of $A$ corresponding to the
	columns of $B$ with pivots form a basis of the column space.
\end{definition}

\section*{Linear Transformations}

\begin{definition}
	Let $V$ and $W$ be vector spaces, and let $T : V \rightarrow W$ be a function.
	We say $T$ is a \emph{linear transformation} (or a \emph{linear map}, or
	simply that $T$ is \emph{linear}) if for all $\vec{x}, \vec{y} \in V$ and all
	$c \in \R$, we have
	\[
		T(c\vec{x} + \vec{y}) = cT(\vec{x}) + T(\vec{y}).
	\]
	That is, $T$ \emph{respects addition} and \emph{scalar multiplication}.
\end{definition}

\begin{corollary}
	When $T$ is linear,
	\[
		T(a_1 \vec{v}_1 + \cdots + a_n \vec{v}_n) = a_1 T(\vec{v}_1) + \cdots + a_n
		T(\vec{v}_n).
	\]
\end{corollary}

\begin{theorem}
	Let $T : V \rightarrow W$ be a linear transformation. If $U : W \rightarrow X$
	is another linear transformation, then the composition $U \circ T : V
	\rightarrow X$ is also linear. The composition $U \circ T$ is often denoted
	simply $UT$.
\end{theorem}

\begin{theorem}
	If $A \in M_{m \times n}(\R)$ is an $m \times n$ matrix, then the function $T
	: \R^n \rightarrow \R^m$ by $T(\vec{x}) = A\vec{x}$ is linear.
\end{theorem}

\begin{theorem}
	If $T : \R^n \rightarrow \R^m$ is linear, then there is a matrix $A \in M_{m
	\times n}(\R)$ so that $T$ is given by $T(\vec{x}) = A\vec{x}$.
\end{theorem}

\begin{definition}
	Let $T : V \rightarrow W$ be a linear transformation.
	\begin{itemize}
		\item The \emph{kernel} or \emph{nullspace} of $T$ is
			\[
				\{\vec{v} \in V \mid T(\vec{v}) = \vec{0}_W\} \subseteq V.
			\]
			It is usually denoted as $\Ker(T)$ or $N(T)$. Its dimension
			$\dim(\Ker(T))$ is called the \emph{nullity} of $T$, sometimes denoted
			$\nullity(T)$.
		\item The \emph{image} or \emph{range} of $T$ is
			\[
				\{T(\vec{v}) \mid \vec{v} \in V\} = \{\vec{w} \in W \mid \exists \vec{v}
				\in V \text{ s.t. } T(\vec{v}) = \vec{w}\} \subseteq W.
			\]
			It is usually denoted as $\Image(T)$ or $R(T)$. Its dimension
			$\dim(\Image(T))$ is called the \emph{rank} of $T$, sometimes denoted
			$\rank(T)$. If $T : \R^n \rightarrow \R^m$ is multiplication by a matrix
			$A$, the range of $T$ is sometimes called the \emph{column space} of $A$,
			because it is precisely the span of the columns of $A$.
	\end{itemize}
\end{definition}

\begin{facts}
	Let $T : V \rightarrow W$ be a linear transformation.
	\begin{itemize}
		\item $N(T)$ is a subspace of $V$, and $R(T)$ is a subspace of $W$.
		\item $T$ is one-to-one if and only if $N(T) = \{\vec{0}_V\}$, i.e., iff
			$N(T)$ is as small as possible, i.e., iff $\nullity(T) = 0$.
		\item $T$ is onto if and only if $R(T) = W$, i.e., iff $R(T)$ is as big as
			possible. If $\dim(W) < \infty$, this is equivalent to saying $\rank(T) =
			\dim(W)$.
		\item If $T : \R^n \rightarrow \R^m$ is represented by a matrix $A$, then
			\[
				N(T) = \{\vec{x} \in \R^n \mid A\vec{x} = \vec{0}\} \text{ and } R(T) =
				\text{column space of $A$}.
			\]
			Furthermore, after finding an echelon form of $A$ via row reduction,
			$\rank(T)$ is the number of columns \emph{with} pivots (since the
			corresponding columns form a basis of $R(T)$), and $\nullity(T)$ is the
			number of columns \emph{without} pivots (since the vectors that give the
			general solution form a basis of $N(T)$).
	\end{itemize}
\end{facts}

\begin{fact}
	If $X \subseteq V$ is a subspace, then $\dim(X) \leq \dim(V)$.  Moreover, if
	$\dim(V) < \infty$, then $\dim(X) = \dim(V)$ if and only if $X = V$.
\end{fact}

\begin{fact}
	Let $V$ be a vector space with $\dim(V) = n$, and let $S \subseteq V$ be a set
	of $m$ distinct vectors in $V$.
	\begin{itemize}
		\item If $m < n$, then $S$ cannot span $V$.
		\item If $m > n$, then $S$ cannot be linearly independent.
	\end{itemize}
\end{fact}

\begin{theorem}
	Let $V$ be a vector space, and let $S \subseteq V$ be a set of $n$ distinct
	vectors in $V$. If any two of the following conditions hold, then all three
	hold (and $S$ is a basis for $V$):
	\begin{enumerate}
		\item $S$ is linearly independent.
		\item $S$ spans $V$.
		\item $\dim(V) = n$.
	\end{enumerate}
\end{theorem}

\begin{theorem}
	Let $T : V \rightarrow W$ be a linear transformation, and suppose that at
	least one of $V, W$ is finite-dimensional. If any two of the following
	conditions hold, then all three hold:
	\begin{enumerate}
		\item $T$ is one-to-one.
		\item $T$ is onto.
		\item $\dim(V) = \dim(W)$.
	\end{enumerate}
	In this case, $T$ is \emph{invertible}.
\end{theorem}

\begin{theorem}[Rank-Nullity/Dimension Theorem]
	Let $T : V \rightarrow W$ be a linear transformation. Then
	\[
		\rank(T) + \nullity(T) = \dim(V).
	\]
\end{theorem}

\section*{Matrices}

\begin{facts}
	Some facts about matrices:
	\begin{itemize}
		\item Matrix addition is commutative and associative.
		\item Matrix multiplication is associative \emph{but not commutative}.
		\item The $m \times n$ matrix of zeros (denoted 0 or $O$) is the additive
			identity: $A + O = O + A = A$.
		\item The $n \times n$ identity matrix $I$, sometimes denoted $I_n$, has 1's
			down the diagonal and 0's everywhere else.
		\item The distributive laws: $A(B + C) = AB + AC$ and $(A + B)C = AC + BC$.
	\end{itemize}

	If $A$ is a \emph{square} matrix (i.e., $n \times n$):
	\begin{itemize}
		\item $A$ is a \emph{diagonal matrix} if every entry not on the diagonal is
			0.
		\item The \emph{transpose} of $A$, denoted $A^t$ or $A^T$, is the $n \times
			n$ matrix formed by reflecting $A$ across the diagonal.
		\item The \emph{determinant} of $A$, denoted $\det(A)$, is a real number
			given by a more complicated formula.
			\begin{itemize}
				\item For a given $n \times n$ matrix $A$ where $n \geq 1$,
					\[
						\det(A) = \sum_{j = 1}^n {(-1)}^{i + j} a_{ij} \det(A_{ij}),
					\]
					where $a_{ij}$ is the entry in the $i$th row and $j$th column, and
					$A_{ij}$ is the $(n - 1) \times (n -1)$ matrix obtained by deleting
					the $i$th row and $j$th column of $A$.
				\item $\det(AB) = \det(A) \det(B)$.
				\item $\det(A) \neq 0$ if and only if $A$ is invertible.
				\item $
					\begin{vmatrix}
						a & b \\
						c & d
					\end{vmatrix}$
					means the same thing as
					$\det
					\begin{bmatrix}
						a & b \\
						c & d
					\end{bmatrix}$,
					namely $ad - bc$.
			\end{itemize}
	\end{itemize}
\end{facts}

\begin{definition}
	Let $V$ and $W$ be finite-dimensional vector spaces with bases $\alpha =
	\{\vec{v}_1, \dots, \vec{v}_n\}$ and $\beta = \{\vec{w}_1, \dots,
	\vec{w}_m\}$, respectively, and let $T : V \rightarrow W$ be a linear
	transformation.

	Find real numbers $a_{ij}$, for $q \leq i \leq m$ and $1 \leq j \leq n$, by
	computing $T(\vec{v}_1), \dots, T(\vec{v}_n)$ and expressing each as a linear
	combination of $\vec{w}_1, \dots, \vec{w}_m$. That is,
	\begin{align*}
		T(\vec{v}_1) &= a_{11} \vec{w}_1 + a_{21} \vec{w}_2 + \cdots + a_{m1}
		\vec{w}_m \\
		T(\vec{v}_2) &= a_{12} \vec{w}_1 + a_{22} \vec{w}_2 + \cdots + a_{m2}
		\vec{w}_m \\
		&\vdotswithin{=} \\
		T(\vec{v}_n) &= a_{1n} \vec{w}_1 + a_{2n} \vec{w}_2 + \cdots + a_{mn}
		\vec{w}_m.
	\end{align*}
	Then, turning the grid of coefficients sideways, the \emph{$m \times n$ matrix
	of $T$ with respect to $\alpha$ and $\beta$} is
	\[
		{[T]}_\alpha^\beta =
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			a_{21} & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{m1} & a_{m2} & \cdots & a_{mn} \\
		\end{bmatrix}.
	\]
\end{definition}

\begin{definition}
	Let $V$ be a finite-dimensional vector space, and let $\alpha = \{\vec{v}_1,
	\dots, \vec{v}_n\}$ be a basis for $V$. For any $\vec{v} \in V$, the
	\emph{coordinate vector} of $\vec{v}$ with respect to $\alpha$ is the
	$n$-entry column vector ${[\vec{v}]}_\alpha =
	\begin{bmatrix}
		a_1 \\
		\vdots \\
		a_n
	\end{bmatrix} \in \R^n$,
	where $a_1, \dots, a_n \in \R$ are the unique scalars such that $\vec{v} = a_1
	\vec{v}_1 + \cdots + a_n \vec{v}_n$.
\end{definition}

\begin{fact}
	For any $\vec{v} \in V$, the coordinate vectors ${[\vec{v}]}_\alpha \in \R^n$
	and ${[T(\vec{v})]}_\beta \in \R^m$ are related via the matrix
	${[T]}_\alpha^\beta \in M_{m \times n}(\R)$ by the equation
	\[
		{[T]}_\alpha^\beta {[\vec{v}]}_\alpha = {[T(\vec{v})]}_\beta.
	\]
\end{fact}

\begin{fact}
	If $X$ is another vector space, with basis $\gamma = \{\vec{x}_1, \dots,
	\vec{x}_p\}$, and if $U : W \rightarrow X$ is linear, then the $m \times n$
	matrix ${[T]}_\alpha^\beta$, the $n \times p$ matrix ${[U]}_\beta^\gamma$, and
	the $m \times p$ matrix ${[UT]}_\alpha^\gamma$ are related by matrix
	multiplication:
	\[
		{[UT]}_\alpha^\beta = {[U]}_\beta^\gamma {[T]}_\alpha^\beta.
	\]
\end{fact}

\begin{definition}
	A linear transformation $T : V \rightarrow W$ is \emph{invertible} if it has
	an \emph{inverse}, i.e., if there is another linear map $T^{-1} : W
	\rightarrow V$ such that $T(T^{-1}(\vec{w})) = \vec{w}$ for all $\vec{w} \in
	W$, and $T^{-1}(T(\vec{v})) = \vec{v}$ for all $\vec{v} \in V$. The following
	are equivalent:
	\begin{itemize}
		\item $T$ is invertible (i.e., $T$ has an inverse $T^{-1} : W \rightarrow
			V$).
		\item $T$ is one-to-one and onto.
	\end{itemize}
\end{definition}

\begin{facts}
	Suppose $T: V \rightarrow W$ is invertible. Then:
	\begin{itemize}
		\item Its inverse $T^{-1}$ is unique.
		\item Its inverse $T^{-1}$ is also invertible, and ${(T^{-1})}^{-1} = T$.
		\item If $U : W \rightarrow X$ is also invertible, then $UT : V \rightarrow
			X$ is invertible, and ${(UT)}^{-1} = T^{-1} U^{-1}$.
	\end{itemize}
\end{facts}

\begin{definition}
	A square matrix $A \in M_{n \times n}(\R)$ is \emph{invertible} if it has an
	\emph{inverse}, i.e., if there is another matrix $B \in M_{n \times n}(\R)$
	such that $AB = I$ and $BA = I$. The following are equivalent:
	\begin{itemize}
		\item $A$ is invertible.
		\item The columns of $A$ are linearly independent.
		\item The rows of $A$ are linearly independent.
		\item The columns of $A$ together span $\R^n$.
		\item The rows of $A$ together span $\R^n$.
		\item The column space (i.e., range or image) of $A$ is all of $R^n$.
		\item The null space (i.e., kernel) of $A$ is $\{\vec{0}\}$.
		\item $\rank(A) = n$.
		\item $\nullity(A) = 0$.
		\item $\det(A) \neq 0$.
		\item $\lambda = 0$ is not an eigenvalue of $A$.
	\end{itemize}
\end{definition}

\begin{facts}
	Suppose $A \in M_{n \times n}(\R)$ is invertible. Then:
	\begin{itemize}
		\item Its inverse $A^{-1}$ is unique.
		\item Its inverse $A^{-1}$ is also invertible, and ${(A^{-1})}^{-1} = A$.
		\item If $B \in M_{n \times n}(\R)$ is invertible, then $AB$ is invertible,
			and ${(AB)}^{-1} = B^{-1} A^{-1}$.
	\end{itemize}
\end{facts}

\begin{facts}
	Computing inverse:
	\begin{itemize}
		\item $A =
			\begin{bmatrix}
				a & b \\
				c & d
			\end{bmatrix}$
			is invertible if and only if $ad - bc \neq 0$, in which case $A^{-1} =
			\frac{1}{ad - bc}
			\begin{bmatrix}
				d & -b \\
				-c & a
			\end{bmatrix}$.
		\item For $3 \times 3$ and larger matrices, use the Gauss-Jordan method (row
			reduction).
	\end{itemize}
\end{facts}

\begin{definition}
	Let $V$ be a finite-dimensional vector space, and let $\alpha = \{\vec{v}_1,
	\dots, \vec{v}_n\}$ and $\beta = \{\vec{w}_1, \dots, \vec{w}_n\}$ both be
	bases for $V$. The \emph{change-of-basis matrix} from the $\alpha$-coordinate
	to $\beta$-coordinate is the $n \times n$ matrix ${[I]}_\alpha^\beta$, where
	$I: V \rightarrow V$ is the identity map $I(\vec{v}) = \vec{v}$. That is,
	\[
		{[I]}_\alpha^\beta =
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			a_{21} & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{n1} & a_{n2} & \cdots & a_{nn}
		\end{bmatrix},
	\]
	where
	\begin{align*}
		\vec{v}_1 &= a_{11} \vec{w}_1 + a_{21} \vec{w}_2 + \cdots + a_{m1} \vec{w}_m
		\\
		\vec{v}_2 &= a_{12} \vec{w}_1 + a_{22} \vec{w}_2 + \cdots + a_{m2} \vec{w}_m
		\\
		&\vdotswithin{=} \\
		\vec{v}_n &= a_{1n} \vec{w}_1 + a_{2n} \vec{w}_2 + \cdots + a_{mn}
		\vec{w}_m.
	\end{align*}
	As before, note that the grid of coefficients is flipped sideways to form the
	matrix.
\end{definition}

\begin{fact}
	For any $\vec{v} \in V$, recall that ${[\vec{v}]}_\alpha \in \R^n$ is the
	$n$-entry column vector of $\alpha$-coordinates for $\vec{v}$. We can compute
	the $\beta$-coordinate vector for $\vec{v}$ via the formula ${[\vec{v}]}_\beta
	= {[I]}_\alpha^\beta {[\vec{v}]}_\alpha$.
\end{fact}

\begin{fact}
	The change of coordinates matrix to change coordinates the other way is the
	inverse: ${[I]}_\beta^\alpha = {({[I]}_\alpha^\beta)}^{-1}$.
\end{fact}
