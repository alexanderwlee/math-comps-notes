\part*{Linear Algebra}

\section*{Vector Spaces and Subspaces}

\begin{definition}
	A (real) \emph{vector space} is a set $V$ (whose elements are called
	\emph{vectors}) together with
	\begin{enumerate}
		\item an operation called \emph{vector addition}, which for each pair of
			vector $\vec{x}, \vec{y} \in V$ produces another vector in $V$ denoted
			$\vec{x} + \vec{y}$, and
		\item an operation called \emph{multiplication by a scalar} (a real number),
			which for each vector $\vec{x} \in V$, and each scalar $c \in \R$
			produces another vector in $V$ denoted $c \vec{x}$.
	\end{enumerate}
	Furthermore, the two operations must satisfy the follow \emph{axioms}:
	\begin{enumerate}
		\item For all vectors $\vec{x}, \vec{y}$, and $\vec{z} \in V$, $(\vec{x}
			+ \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z})$.
		\item For all vectors $\vec{x}$ and $\vec{y} \in V$, $\vec{x} + \vec{y}
			= \vec{y} + \vec{x}$.
		\item There exists a vector $\vec{0} \in V$ with the property that
			$\vec{x} + \vec{0} = \vec{x}$ for all vectors $\vec{x} \in V$.
		\item For each vector $\vec{x} \in V$, there exists a vector denoted
			$-\vec{x}$ with the property that $\vec{x} + -\vec{x} = \vec{0}$.
		\item For all vectors $\vec{x}$ and $\vec{y} \in V$ and all scalars $c \in
			\R$, $c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$.
		\item For all vectors $\vec{x} \in V$, and all scalars $c$ and $d \in \R$,
			$(c + d)\vec{x} = c\vec{x} + d\vec{x}$.
		\item For all vectors $\vec{x} \in V$, and all scalars $c$ and $d \in \R$,
			$(cd)\vec{x} = c(d\vec{x})$.
		\item For all vectors $\vec{x} \in V$, $1\vec{x} = \vec{x}$.
	\end{enumerate}
\end{definition}

\begin{examples}
	Some simple vector spaces:
	\begin{itemize}
		\item $\R^n$ is the vector space of ordered $n$-tuples of real numbers.
			Note: $\dim(\R^n) = n$.
		\item $P_n(\R)$ is the vector space of polynomials of degree \emph{less
			than or equal to $n$}. Note: $\dim(P_n(\R)) = n + 1$.
		\item $M_{m \times n}(\R)$ is the vector space of $m \times n$ matrices with
			real entries. Note: $\dim(M_{m \times n}(\R)) = mn$.
	\end{itemize}
\end{examples}

\begin{definition}
	Let $V$ be a vector space and let $W \subseteq V$ be a subset. Then $W$ is a
	(vector) \emph{subspace} of $V$ if $W$ is a vector space itself under the
	operations of vector sum and scalar multiplication from $V$.
\end{definition}

\begin{notes}
	The empty set $\emptyset$ is not a vector space. Instead the smallest vector
	space is the trivial space, $\{\vec{0}\}$. Every vector space $V$ has two
	obvious subspaces: the trivial subspace $\{\vec{0}\} \subseteq V$, and the
	improper subspace $V \subseteq V$.
\end{notes}

\begin{theorem}[Subspace Theorem]
	Let $V$ be a vector space. A subset $W \subseteq V$ is a subspace if it
	satisfies the following properties:
	\begin{enumerate}
		\item $W \neq \emptyset$
		\item For all $\vec{x}, \vec{y} \in W$ and all $c \in \R$, we have $c\vec{x} +
			\vec{y} \in W$.
	\end{enumerate}
\end{theorem}

\begin{definition}
	Let $V$ be a vector space, and let $S = \{\vec{v}_1, \dots, \vec{v}_n\}
	\subseteq V$ be a finite set of vectors in $V$.
	\begin{itemize}
		\item A \emph{linear combination} of elements of $S$ is an expression $a_1
			\vec{v}_1 + \cdots + a_n \vec{v}_n$ for some scalars $a_1, \dots, a_n
			\in \R$.
		\item The \emph{span} of $S$, denoted $\Span(S)$, is the set of all linear
			combinations of elements of $S$. That is, \[\Span(S) = \{a_1 \vec{v}_1
			+ \cdots a_n \vec{v}_n \mid a_1, \dots, a_n \in \R\}.\]
		\item We define $\Span(\emptyset) = \{\vec{0}\}$.
		\item If $\Span(S) = W$, we say that $S$ spans $W$.
	\end{itemize}
\end{definition}

\begin{fact}
	Let $V$ be a vector space and let $S$ be any subset of $V$. Then
	$\Span(S)$ is a subspace of $V$.
\end{fact}

\begin{fact}
	If $W$ is a subspace and $S \subseteq W$, then $\Span(S) \subseteq W$.
\end{fact}

\begin{definition}
	The set $S$ is \emph{linearly dependent} if there exists scalars $a_1, \dots,
	a_n \in \R$ that are not all zero such that $a_1 \vec{v}_1 + \cdots + a_n
	\vec{v}_n = \vec{0}$. $S$ is \emph{linearly independent} if it is not
	linearly dependent. Equivalently, for any scalars $a_1, \dots, a_n \in \R$
	such that $a_1 \vec{v}_1 + \cdots a_n \vec{v}_n = \vec{0}$, we must have
	$a_1 = \cdots = a_n = 0$.
\end{definition}

\begin{definition}
	The set $S \subseteq V$ is a basis for $V$ if $S$ is linearly independent
	and $\Span(S) = V$.
\end{definition}

\begin{definition}
	The \emph{dimension} of $V$ is the number $\dim(V)$ of elements in a basis
	for $V$. If $V$ has no finite basis, we say $\dim(V) = \infty$.
\end{definition}

\begin{theorem}
	Any two bases of $V$ have the same number of elements.
\end{theorem}

\begin{fact}
	The three kinds of row reduction steps are
	\begin{enumerate}
		\item Switching two rows.
		\item Multiplying a row by a nonzero scalar.
		\item Adding a multiple of one row to another.
	\end{enumerate}
\end{fact}

\begin{definition}
	A matrix is in \emph{echelon form} if it satisfies all of the following
	conditions:
	\begin{enumerate}
		\item If a row is not a zero row (i.e., all entries of that row are zeros),
			then the first nonzero entry is a 1 (and called the \emph{pivot}).
		\item If a column contains a pivot, then all other entries in that column
			are 0.
		\item If a row contains a pivot, then each row above contains a pivot
			further to the left. This also implies that zero rows, if any, appear at
			the bottom.
	\end{enumerate}
	Variables corresponding to the pivots are called \emph{pivot variables}.
	All other variables are called \emph{free variables}.
\end{definition}

\begin{definition}
	A \emph{homogeneous} system of linear equations is when all the linear
	combinations equal 0. A system is \emph{inhomogeneous} otherwise.
\end{definition}

\begin{definition}
	The \emph{nullspace} of a matrix $A$ is the solution set of its
	corresponding homogeneous system of equations. The basis of the nullspace
	of $A$ is the set of vectors that the free variables end up multiplied by in
	the solution.
\end{definition}

\begin{definition}
	The \emph{column space} of a matrix $A$ is the span of its columns. If $B$ is
	the echelon form of $A$, then the columns of $A$ corresponding to the
	columns of $B$ with pivots form a basis of the column space.
\end{definition}

\section*{Linear Transformations}

\begin{definition}
	Let $V$ and $W$ be vector spaces, and let $T : V \rightarrow W$ be a function.
	We say $T$ is a \emph{linear transformation} (or a \emph{linear map}, or
	simply that $T$ is \emph{linear}) if for all $\vec{x}, \vec{y} \in V$ and all
	$c \in \R$, we have
	\[
		T(c\vec{x} + \vec{y}) = cT(\vec{x}) + T(\vec{y}).
	\]
	That is, $T$ \emph{respects addition} and \emph{scalar multiplication}.
\end{definition}

\begin{corollary}
	When $T$ is linear,
	\[
		T(a_1 \vec{v}_1 + \cdots + a_n \vec{v}_n) = a_1 T(\vec{v}_1) + \cdots + a_n
		T(\vec{v}_n).
	\]
\end{corollary}

\begin{theorem}
	Let $T : V \rightarrow W$ be a linear transformation. If $U : W \rightarrow X$
	is another linear transformation, then the composition $U \circ T : V
	\rightarrow X$ is also linear. The composition $U \circ T$ is often denoted
	simply $UT$.
\end{theorem}

\begin{theorem}
	If $A \in M_{m \times n}(\R)$ is an $m \times n$ matrix, then the function $T
	: \R^n \rightarrow \R^m$ by $T(\vec{x}) = A\vec{x}$ is linear.
\end{theorem}

\begin{theorem}
	If $T : \R^n \rightarrow \R^m$ is linear, then there is a matrix $A \in M_{m
	\times n}(\R)$ so that $T$ is given by $T(\vec{x}) = A\vec{x}$.
\end{theorem}

\begin{definition}
	Let $T : V \rightarrow W$ be a linear transformation.
	\begin{itemize}
		\item The \emph{kernel} or \emph{nullspace} of $T$ is
			\[
				\{\vec{v} \in V \mid T(\vec{v}) = \vec{0}_W\} \subseteq V.
			\]
			It is usually denoted as $\Ker(T)$ or $N(T)$. Its dimension
			$\dim(\Ker(T))$ is called the \emph{nullity} of $T$, sometimes denoted
			$\nullity(T)$.
		\item The \emph{image} or \emph{range} of $T$ is
			\[
				\{T(\vec{v}) \mid \vec{v} \in V\} = \{\vec{w} \in W \mid \exists \vec{v}
				\in V \text{ s.t. } T(\vec{v}) = \vec{w}\} \subseteq W.
			\]
			It is usually denoted as $\Image(T)$ or $R(T)$. Its dimension
			$\dim(\Image(T))$ is called the \emph{rank} of $T$, sometimes denoted
			$\rank(T)$. If $T : \R^n \rightarrow \R^m$ is multiplication by a matrix
			$A$, the range of $T$ is sometimes called the \emph{column space} of $A$,
			because it is precisely the span of the columns of $A$.
	\end{itemize}
\end{definition}

\begin{facts}
	Let $T : V \rightarrow W$ be a linear transformation.
	\begin{itemize}
		\item $N(T)$ is a subspace of $V$, and $R(T)$ is a subspace of $W$.
		\item $T$ is one-to-one if and only if $N(T) = \{\vec{0}_V\}$, i.e., iff
			$N(T)$ is as small as possible, i.e., iff $\nullity(T) = 0$.
		\item $T$ is onto if and only if $R(T) = W$, i.e., iff $R(T)$ is as big as
			possible. If $\dim(W) < \infty$, this is equivalent to saying $\rank(T) =
			\dim(W)$.
		\item If $T : \R^n \rightarrow \R^m$ is represented by a matrix $A$, then
			\[
				N(T) = \{\vec{x} \in \R^n \mid A\vec{x} = \vec{0}\} \text{ and } R(T) =
				\text{column space of $A$}.
			\]
			Furthermore, after finding an echelon form of $A$ via row reduction,
			$\rank(T)$ is the number of columns \emph{with} pivots (since the
			corresponding columns form a basis of $R(T)$), and $\nullity(T)$ is the
			number of columns \emph{without} pivots (since the vectors that give the
			general solution form a basis of $N(T)$).
	\end{itemize}
\end{facts}

\begin{fact}
	If $X \subseteq V$ is a subspace, then $\dim(X) \leq \dim(V)$.  Moreover, if
	$\dim(V) < \infty$, then $\dim(X) = \dim(V)$ if and only if $X = V$.
\end{fact}

\begin{fact}
	Let $V$ be a vector space with $\dim(V) = n$, and let $S \subseteq V$ be a set
	of $m$ distinct vectors in $V$.
	\begin{itemize}
		\item If $m < n$, then $S$ cannot span $V$.
		\item If $m > n$, then $S$ cannot be linearly independent.
	\end{itemize}
\end{fact}

\begin{theorem}
	Let $V$ be a vector space, and let $S \subseteq V$ be a set of $n$ distinct
	vectors in $V$. If any two of the following conditions hold, then all three
	hold (and $S$ is a basis for $V$):
	\begin{enumerate}
		\item $S$ is linearly independent.
		\item $S$ spans $V$.
		\item $\dim(V) = n$.
	\end{enumerate}
\end{theorem}

\begin{theorem}
	Let $T : V \rightarrow W$ be a linear transformation, and suppose that at
	least one of $V, W$ is finite-dimensional. If any two of the following
	conditions hold, then all three hold:
	\begin{enumerate}
		\item $T$ is one-to-one.
		\item $T$ is onto.
		\item $\dim(V) = \dim(W)$.
	\end{enumerate}
	In this case, $T$ is \emph{invertible}.
\end{theorem}

\begin{theorem}[Rank-Nullity/Dimension Theorem]
	Let $T : V \rightarrow W$ be a linear transformation. Then
	\[
		\rank(T) + \nullity(T) = \dim(V).
	\]
\end{theorem}
